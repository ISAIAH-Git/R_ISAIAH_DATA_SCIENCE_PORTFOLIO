{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF: TERM FREQUENCY INVERSE DOCUMENT FREQUENCY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have wrestled with an alligator. That’s right; I have wrestled with an alligator. I have tussled with a whale. I have handcuffed lightning, thrown thunder in jail. That’s bad. Only last week I murdered a rock; injured a stone; hospitalised a brick. I’m so mean I make medicine sick…. Fast, fast, fast, last night I cut the light off in the bedroom, hit the switch and hit the bed before the room was dark. Fast. All of you chumps are going to bow when I whoop him. I know you got him picked. But when the man is in trouble – “I’m a show you how great I am!”\n"
     ]
    }
   ],
   "source": [
    "paragraph=\"\"\"I have wrestled with an alligator. That’s right; I have wrestled with an alligator. I have tussled with a whale. I have handcuffed lightning, thrown thunder in jail. That’s bad. Only last week I murdered a rock; injured a stone; hospitalised a brick. I’m so mean I make medicine sick…. Fast, fast, fast, last night I cut the light off in the bedroom, hit the switch and hit the bed before the room was dark. Fast. All of you chumps are going to bow when I whoop him. I know you got him picked. But when the man is in trouble – “I’m a show you how great I am!”\"\"\"\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEMMATIZATION : \n",
      "__________________________________________________________________\n",
      "['wrestled alligator.', 'hat’s right; wrestled alligator.', 'tussled whale.', 'handcuffed lightning, thrown thunder jail.', 'hat’s bad.', 'nly last week murdered rock; injured stone; hospitalised brick.', '’m mean make medicine sick….', 'ast, fast, fast, last night cut light bedroom, hit switch hit bed room dark.', 'ast.', 'chump going bow whoop him.', 'know got picked.', 'ut man trouble – “i’m show great am!”']\n",
      "__________________________________________________________________\n",
      "['I have wrestled with an alligator.', 'That’s right; I have wrestled with an alligator.', 'I have tussled with a whale.', 'I have handcuffed lightning, thrown thunder in jail.', 'That’s bad.', 'Only last week I murdered a rock; injured a stone; hospitalised a brick.', 'I’m so mean I make medicine sick….', 'Fast, fast, fast, last night I cut the light off in the bedroom, hit the switch and hit the bed before the room was dark.', 'Fast.', 'All of you chumps are going to bow when I whoop him.', 'I know you got him picked.', 'But when the man is in trouble – “I’m a show you how great I am!”']\n"
     ]
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('^[a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)\n",
    "print('LEMMATIZATION : ')\n",
    "print('__________________________________________________________________')\n",
    "print(corpus)\n",
    "print('__________________________________________________________________')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAG OF WORDS OF THE PARAGRAPH : \n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1\n",
      "  0 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1 0 0 0 1 1 2 0 0 0 0 0 0 2 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 1 0 1 0 0 0 0]]\n",
      "(12, 50)\n"
     ]
    }
   ],
   "source": [
    "#COMPARING TO BAG OF WORDS \n",
    "#Creating the bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "print('BAG OF WORDS OF THE PARAGRAPH : ')\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF OF THE PARAGRAPH : \n",
      "[[0.70710678 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.70710678]\n",
      " [0.47914251 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.47914251 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5579134  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.47914251]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.70710678\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.4472136  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.75863071 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.65152087 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33830232 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33830232 0.33830232 0.         0.         0.29053796\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33830232 0.         0.33830232 0.         0.         0.33830232\n",
      "  0.         0.         0.         0.33830232 0.         0.\n",
      "  0.         0.         0.         0.         0.33830232 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.5        0.5\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.20544136 0.         0.23921587 0.23921587\n",
      "  0.         0.         0.         0.23921587 0.23921587 0.47843173\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47843173 0.         0.         0.         0.         0.20544136\n",
      "  0.23921587 0.         0.         0.         0.         0.\n",
      "  0.         0.23921587 0.         0.         0.         0.\n",
      "  0.23921587 0.         0.         0.         0.23921587 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.4472136  0.         0.4472136  0.         0.         0.\n",
      "  0.4472136  0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.40824829 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.40824829 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.40824829 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.40824829 0.         0.         0.         0.\n",
      "  0.         0.40824829 0.         0.40824829 0.         0.\n",
      "  0.         0.        ]]\n",
      "(12, 50)\n"
     ]
    }
   ],
   "source": [
    "#Creating the TFIDF VECTOR\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer()\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "print('TFIDF OF THE PARAGRAPH : ')\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion in the Bag Of words we see only binary classification\n",
    "#Where  as TFIDF signify the word of importance in the sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
